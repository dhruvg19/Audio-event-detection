{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import librosa\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import glob as glob\n",
    "from numpy import asarray\n",
    "from numpy import save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAudio(filename):\n",
    "    x, sr = librosa.load(filename, sr=16000)\n",
    "    return x, sr\n",
    "\n",
    "#calculate spectrogram\n",
    "def calc_spec(x):\n",
    "    n_fft = 1024\n",
    "    hop_length = 512\n",
    "    win_length = 1024\n",
    "    X = np.abs(librosa.stft(x, n_fft = n_fft, hop_length = hop_length, win_length = win_length, window='hann', dtype = np.clongdouble))\n",
    "    X = librosa.power_to_db(X**2,ref=np.max)\n",
    "    return X\n",
    "\n",
    "def saveSpectrogram(X, outfilename):\n",
    "    assert outfilename[-4:]=='.npy'  #'outfilename extension should be .npy'\n",
    "    np.save(outfilename, X)\n",
    "    return\n",
    "\n",
    "def readSpectrogram(infilename):\n",
    "    X = np.load(infilename)\n",
    "    return X\n",
    "\n",
    "nmfcc = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr):          # used to scale all values between 0 and 1\n",
    "    arr_max = np.ndarray.max(arr, axis = 1)    # obtain max and min values for each feature   \n",
    "    arr_min = np.ndarray.min(arr, axis = 1)\n",
    "    arr_max = arr_max.reshape((nmfcc, 1))\n",
    "    arr_min = arr_min.reshape((nmfcc, 1))\n",
    "    diff = arr_max - arr_min         # obtain range of each feature\n",
    "    arr = arr-arr_min\n",
    "    arr = arr/diff\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def listofsamples(music_mfcc):         # create a list of all the samples\n",
    "    samples = []\n",
    "    cols = music_mfcc.shape[1]        # total no of samples\n",
    "    for i in range(cols):\n",
    "        samples.append(music_mfcc[:, i])\n",
    "\n",
    "    arr = np.array(samples)     # convert list to 2d array\n",
    "    arr = arr.T\n",
    "    arr = normalize(arr)         # spread values between 0 and 1\n",
    "    cols = arr.shape[1]\n",
    "    n_samples = []\n",
    "    for i in range(cols):\n",
    "        n_samples.append(arr[:, i])    # list of normalized samples\n",
    "    return n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54392\n",
      "(13,)\n",
      "[0.67890245 0.5774692  0.40413493 0.45244664 0.47015077 0.4663162\n",
      " 0.7861795  0.49941975 0.7575369  0.3286484  0.22860876 0.46127218\n",
      " 0.609776  ]\n"
     ]
    }
   ],
   "source": [
    "dirname = 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/Datasets/SpeechData/Speech2.wav'\n",
    "speech, sr = readAudio(dirname)\n",
    "Speech = librosa.feature.mfcc(y=speech, sr= 16000, n_mfcc= nmfcc)\n",
    "speech = listofsamples(Speech)       #Speech Dataset\n",
    "print(len(speech))\n",
    "print(speech[0].shape)\n",
    "print(speech[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56211\n",
      "(13,)\n"
     ]
    }
   ],
   "source": [
    "dirname = 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/Datasets/MusicData/music2.wav'\n",
    "music, sr = readAudio(dirname)\n",
    "Music = librosa.feature.mfcc(y=music, sr= 16000, n_mfcc= nmfcc)\n",
    "music = listofsamples(Music)    #Music Dataset\n",
    "print(len(music))\n",
    "print(music[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62501\n",
      "(13,)\n"
     ]
    }
   ],
   "source": [
    "dirname = 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/Datasets/NoiseData/noise2.wav'\n",
    "noise, sr = readAudio(dirname)\n",
    "Noise = librosa.feature.mfcc(y=noise, sr= 16000, n_mfcc= nmfcc)\n",
    "silence = listofsamples(Noise)      #Silence Dataset\n",
    "print(len(silence))\n",
    "print(silence[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56211\n",
      "54392\n",
      "62501\n",
      "[0.01581264 0.4047906  0.34330437 0.40189147 0.35940826 0.43214425\n",
      " 0.41150963 0.4052104  0.4465698  0.51538366 0.4778128  0.37968874\n",
      " 0.4904235 ]\n"
     ]
    }
   ],
   "source": [
    "# print(len(music))      just functions to confirm shapes\n",
    "# print(len(speech))\n",
    "# print(len(silence))\n",
    "# print(silence[56])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Mixture Models implementation:-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GMM:\n",
    "    def __init__(self, max_iter = 50):\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "        # pi list contains the fraction of the dataset for every cluster\n",
    "        self.pi = [1/3 for comp in range(3)]\n",
    "        \n",
    "    def multivariate_normal(self, X, mean_vector, covariance_matrix):     # implements pdf of multivariate normal distribution\n",
    "        (sign, logdet) = np.linalg.slogdet(covariance_matrix)      # used to find det of matrix without overflowing\n",
    "        if logdet < -100:\n",
    "            det = np.exp(-5)\n",
    "        else:    \n",
    "            det = np.exp(logdet)\n",
    "        \n",
    "        a = covariance_matrix\n",
    "        m = 10^-6\n",
    "        inv_prod = np.linalg.inv(a+ np.eye(a.shape[1])*m)     #small noise added in diagonals to prevent singularity\n",
    "        \n",
    "        return (2*np.pi)**(-len(X)/2)*det**(-1/2)*np.exp(-np.dot(np.dot((X-mean_vector).T, inv_prod), (X-mean_vector))/2)\n",
    "        \n",
    "    def fit(self, X):\n",
    "        # Spliting the data in 3 sub-sets\n",
    "        temp_X = np.array_split(X, 3)\n",
    "        new_X = []\n",
    "        for x in temp_X:\n",
    "            new_X.append(x.T)\n",
    "            \n",
    "        # Initial computation of the mean-vector and covarience matrix\n",
    "        self.mean_vector = [np.mean(x, axis=1) for x in new_X]\n",
    "        self.covariance_matrixes = [np.cov(x) for x in new_X]\n",
    "        \n",
    "        # Deleting the new_X matrix because we will not need it anymore\n",
    "        del new_X\n",
    "        count = 1\n",
    "        for iteration in range(self.max_iter):\n",
    "            ''' ----------------   E - STEP   ------------------ '''\n",
    "            # Initiating the r matrix, every row contains the probabilities\n",
    "            # for every cluster for this row\n",
    "            self.r = np.ones((len(X), 3))*(1/3)\n",
    "            # Calculating the r matrix\n",
    "            for n in range(len(X)):\n",
    "                for k in range(3):\n",
    "                    self.r[n][k] = self.pi[k] * self.multivariate_normal(X[n], self.mean_vector[k], self.covariance_matrixes[k])\n",
    "                    temp_sum = sum([self.pi[j]*self.multivariate_normal(X[n], self.mean_vector[j], self.covariance_matrixes[j]) for j in range(3)])\n",
    "                    self.r[n][k] /= temp_sum\n",
    "                    \n",
    "            # Calculating the N\n",
    "            N = np.sum(self.r, axis=0)\n",
    "            \n",
    "            \n",
    "            ''' ---------------   M - STEP   --------------- '''\n",
    "            # Initializing the mean vector as a zero vector\n",
    "            self.mean_vector = [np.zeros((len(X[0]),)) for k in range(3)]\n",
    "            # Updating the mean vector\n",
    "            for k in range(3):\n",
    "                for n in range(len(X)):\n",
    "                    if self.r[n][k] < np.exp(-5):\n",
    "                        self.mean_vector[k] = np.exp(-5)\n",
    "                    else:    \n",
    "                        self.mean_vector[k] += self.r[n][k] * X[n]\n",
    "                   \n",
    "                    temp = 1/N[k]\n",
    "                    self.mean_vector[k] = self.mean_vector[k]*temp\n",
    "                    \n",
    "            # Initiating the list of the covariance matrixes\n",
    "            self.covariance_matrixes = [np.zeros((len(X[0]), len(X[0]))) for k in range(3)]\n",
    "            # Updating the covariance matrices\n",
    "            for k in range(3):\n",
    "                for n in range(len(X)):\n",
    "                    diff_vec = X[n]-self.mean_vector[k]\n",
    "                    self.covariance_matrixes[k] += np.dot(diff_vec, diff_vec.T)*self.r[n][k]\n",
    "                    temp = 1/N[k]\n",
    "                    self.covariance_matrixes[k]* temp\n",
    "                \n",
    "            # Updating the pi list\n",
    "            self.pi = [N[k]/len(X) for k in range(3)]\n",
    "            print(count)         #helpful in finding time of training\n",
    "            count = count+1\n",
    "                          \n",
    "    \n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "[0.15395677931312723, 0.07940195395538824, 0.7666412667314995]\n"
     ]
    }
   ],
   "source": [
    "music_model = GMM(50)         #making a GMM model for music\n",
    "music_model.fit(music)\n",
    "print(music_model.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "[0.05212334950320024, 0.3449651118149789, 0.6029115386817304]\n"
     ]
    }
   ],
   "source": [
    "speech_model = GMM(50)          #making a GMM model for speech\n",
    "speech_model.fit(speech)\n",
    "print(speech_model.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "[0.26877920834068697, 0.5456196358301244, 0.185601155829187]\n"
     ]
    }
   ],
   "source": [
    "silence_model = GMM(50)         #making a GMM moel for silence\n",
    "silence_model.fit(silence)\n",
    "print(silence_model.pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Predict(test):      #method used to predict the class to which our test sample belongs\n",
    "    cluster = []\n",
    "    for i in range(len(test)):\n",
    "        music_score = 0        #initializing scores of all classes\n",
    "        speech_score = 0\n",
    "        silence_score = 0\n",
    "        \n",
    "        #Using bayes theorem, p(c/x) = (p(x/c)p(c))/p(x)  where c is class and x is given sample\n",
    "        #We assume p(x) to be constant for all classes and\n",
    "        #p(c) to be equal for all (equally likely events)\n",
    "        #Thus claculating p(x/c) for a class which is summation over pi[k]*normal(x/mu_k, sigma_k)\n",
    "        for k in range(3):\n",
    "            music_score += music_model.pi[k] * music_model.multivariate_normal(test[i], music_model.mean_vector[k], music_model.covariance_matrixes[k])\n",
    "            speech_score += speech_model.pi[k] * speech_model.multivariate_normal(test[i], speech_model.mean_vector[k], speech_model.covariance_matrixes[k])\n",
    "            silence_score += silence_model.pi[k] * silence_model.multivariate_normal(test[i], silence_model.mean_vector[k], silence_model.covariance_matrixes[k])\n",
    "        if music_score > speech_score:\n",
    "            if music_score > silence_score:\n",
    "                cluster.append('Music')      # if music is max score\n",
    "            else:\n",
    "                cluster.append('Silence')    # if silence is max score ... and so on\n",
    "        else:\n",
    "            if speech_score > silence_score:\n",
    "                cluster.append('Speech')\n",
    "            else:\n",
    "                cluster.append('Silence')\n",
    "        \n",
    "    return cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audioEventDetect(cluster, window_size = 0.064):     #initial event detection method\n",
    "    super_list = []      #stores all events\n",
    "    cur_list = []        #stores start time, end time and event class\n",
    "    cur_list.append(0)\n",
    "    cur_list.append(window_size)     # end time of first event\n",
    "    cur_class = cluster[0]\n",
    "    cur_list.append(cur_class)\n",
    "    super_list.append(cur_list)\n",
    "    prev_class = cur_class      \n",
    "    cluster = cluster[1:]        #start iterating from 2nd element of cluster\n",
    "    \n",
    "    for cur_class in cluster:\n",
    "        if cur_class == prev_class:     #eg. if music is followed by music \n",
    "            super_list[-1][1] += window_size - 0.032    #end time of current event is increased by window_size - 0.032s\n",
    "        else:\n",
    "            cur_list = super_list[-1]    \n",
    "            new_start = cur_list[1]      #start time of new event is end time of previous event\n",
    "            cur_list = [new_start]\n",
    "            cur_list.append((new_start + window_size - 0.032))     # end time of new event is new start event + remaining part of overlapping window\n",
    "            cur_list.append(cur_class)\n",
    "            super_list.append(cur_list)\n",
    "        prev_class = cur_class\n",
    "                            \n",
    "    return super_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Music', 'Music', 'Music', 'Music', 'Music', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Music', 'Speech', 'Speech', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Speech', 'Speech', 'Music', 'Music', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Music', 'Music', 'Music', 'Music', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Music', 'Music', 'Music', 'Music', 'Speech', 'Music', 'Music', 'Music', 'Music', 'Music', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Music', 'Music', 'Music', 'Speech', 'Music', 'Music', 'Music', 'Speech', 'Speech', 'Speech', 'Music', 'Music', 'Music', 'Speech', 'Speech', 'Speech', 'Speech', 'Music', 'Music', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Music', 'Speech', 'Music', 'Music', 'Speech', 'Music', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Music', 'Music', 'Music', 'Speech', 'Speech', 'Music', 'Speech', 'Speech', 'Music', 'Music', 'Speech', 'Speech', 'Music', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"new_events = []\\nfor event in events:\\n    if (event[1] - event[0]) >= 0.1:\\n        new_events.append(event)\\n#print(new_events)\\nprint('Now')\\nevents = smoothEventDetect(new_events)\\n#print(events)\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirname = 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/Datasets/NoiseData/noise2.wav'    # test_sample\n",
    "x_test, sr = readAudio(dirname)\n",
    "log_mels = calc_spec(x_test)    # in test, these spectrograms would be given\n",
    "mfcc = librosa.feature.mfcc(S = log_mels, sr=16000, n_mfcc=13)   # extract mfcc features from spectrogram\n",
    "test = listofsamples(mfcc)\n",
    "cluster = Predict(test)       # obtain list of framewise predicted classes\n",
    "print(cluster)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = audioEventDetect(cluster)\n",
    "print(events)                #initial audio event detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 9.312000000000001, 'Speech']]\n"
     ]
    }
   ],
   "source": [
    "noOfFrames = 29        # this functions does a majority voting on 28 (sub)frames and allots corresponding label to this new frame (0.96 s in length) of 28 (sub)frames \n",
    "new_cluster = []\n",
    "count = 0 \n",
    "music_score = 0\n",
    "speech_score = 0\n",
    "silence_score = 0\n",
    "for title in cluster:     # count number of music, speech, or silence classes in 28 frames  \n",
    "    if title == 'Music':\n",
    "        music_score+= 1    \n",
    "    elif title == 'Speech':\n",
    "        speech_score+= 1 \n",
    "    else:\n",
    "        silence_score += 1\n",
    "    if count == noOfFrames-1:     # iterate till 28 frames have been counted \n",
    "        count = -1\n",
    "        if music_score > speech_score:\n",
    "            if music_score > silence_score:\n",
    "                new_cluster.append('Music')    # max class is Music\n",
    "            else:\n",
    "                new_cluster.append('Silence')   #max class is Silence.. and so on\n",
    "        else:\n",
    "            if speech_score > silence_score:\n",
    "                new_cluster.append('Speech')\n",
    "            else:\n",
    "                new_cluster.append('Silence') \n",
    "        music_score = 0\n",
    "        speech_score = 0\n",
    "        silence_score = 0\n",
    "    count+= 1\n",
    "    \n",
    "events = audioEventDetect(new_cluster, 0.96)       #pass this new cluster of 0.96s frames through audio event detection method\n",
    "print(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15395677931312723, 0.07940195395538824, 0.7666412667314995]\n"
     ]
    }
   ],
   "source": [
    "# Saving model weights in the form of .npy files\n",
    "dirname = 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/GMM_Weights/music_pi.npy'\n",
    "print(music_model.pi)\n",
    "saveSpectrogram(music_model.pi, dirname)\n",
    "saveSpectrogram(music_model.mean_vector, 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/GMM_Weights/music_mean.npy')\n",
    "saveSpectrogram(music_model.covariance_matrixes, 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/GMM_Weights/music_covariance.npy')\n",
    "saveSpectrogram(speech_model.pi, 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/GMM_Weights/speech_pi.npy')\n",
    "saveSpectrogram(speech_model.mean_vector, 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/GMM_Weights/speech_mean.npy')\n",
    "saveSpectrogram(speech_model.covariance_matrixes, 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/GMM_Weights/speech_cov.npy')\n",
    "saveSpectrogram(silence_model.pi, 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/GMM_Weights/silence_pi.npy')\n",
    "saveSpectrogram(silence_model.mean_vector, 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/GMM_Weights/silence_mean.npy')\n",
    "saveSpectrogram(silence_model.covariance_matrixes, 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/GMM_Weights/silence_cov.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.14926945 0.08193013 0.76880042]\n"
     ]
    }
   ],
   "source": [
    "pi = np.load('D:/Videh_Acads/IITK/5th Sem/EE603/Project/GMM_Weights/music_pi.npy')   # load weights\n",
    "#print(pi)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
