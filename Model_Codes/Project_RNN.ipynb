{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'divide': 'ignore', 'over': 'warn', 'under': 'ignore', 'invalid': 'ignore'}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, LSTM, Reshape\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn import preprocessing\n",
    "from keras.layers.core import Reshape\n",
    "np.seterr(divide='ignore', invalid='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAudio(filename):\n",
    "    x, sr = librosa.load(filename, sr=16000)\n",
    "    return x, sr\n",
    "\n",
    "#calculate spectrogram\n",
    "def calc_spec(x):\n",
    "    n_fft = 1024\n",
    "    hop_length = 512\n",
    "    win_length = 1024\n",
    "    X = np.abs(librosa.stft(x, n_fft = n_fft, hop_length = hop_length, win_length = win_length, window='hann', dtype = np.clongdouble))\n",
    "    X = librosa.power_to_db(X**2,ref=np.max)\n",
    "    return X\n",
    "\n",
    "def saveSpectrogram(X, outfilename):\n",
    "    assert outfilename[-4:]=='.npy'  #'outfilename extension should be .npy'\n",
    "    np.save(outfilename, X)\n",
    "    return\n",
    "\n",
    "def readSpectrogram(infilename):\n",
    "    X = np.load(infilename)\n",
    "    return X\n",
    "\n",
    "nmfcc = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr, features = 513):             # used to scale all values between 0 and 1\n",
    "    arr_max = np.ndarray.max(arr, axis = 1)      #obtaining max and min values for each feature\n",
    "    arr_min = np.ndarray.min(arr, axis = 1)\n",
    "    arr_max = arr_max.reshape((features, 1))\n",
    "    arr_min = arr_min.reshape((features, 1))\n",
    "    diff = arr_max - arr_min                    #getting the range of each value\n",
    "    arr = arr- arr_min\n",
    "    arr = arr/diff \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataset(dirname):            # obtain 3d array (samples, 27, 19)                        \n",
    "    speech, sr = readAudio(dirname)\n",
    "    Speech = calc_spec(speech)          #obtain Spectrogram\n",
    "    Speech = normalize(Speech)\n",
    "    imageNo = Speech.shape[1]         #Number of samples (frames)\n",
    "    X = np.zeros((imageNo, 27, 19))    \n",
    "    for i in range(imageNo):\n",
    "        Image = Speech[:, i]                # storing 513 features into Image \n",
    "        Image = Image.reshape((27, 19))       # reshaping 513 = 27*19 features\n",
    "        X[i] = Image     \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'C:/Users/Dhruv Goyal/Desktop/Ye Khol/music2.wav'\n",
    "X_music = createDataset(dirname)     #Music Dataset\n",
    "imageNo = X_music.shape[0]\n",
    "t1 =  np.array([[1,0,0]]*imageNo)    #Corresponding ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54392, 3)\n",
      "54392\n"
     ]
    }
   ],
   "source": [
    "dirname = 'C:/Users/Dhruv Goyal/Desktop/Ye Khol/Speech2.wav'\n",
    "X_speech = createDataset(dirname)      #Speech Dataset\n",
    "imageNo = X_speech.shape[0]\n",
    "t2 =  np.array([[0,1,0]]*imageNo)  #Corresponding Ground Truths\n",
    "print(t2.shape)\n",
    "print(X_speech.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62501, 3)\n",
      "62501\n"
     ]
    }
   ],
   "source": [
    "dirname = 'C:/Users/Dhruv Goyal/Desktop/Ye Khol/noise2.wav'\n",
    "X_noise = createDataset(dirname)       #Silence Dataset\n",
    "imageNo = X_noise.shape[0]\n",
    "t3 =  np.array([[0,0,1]]*imageNo)      #Corresponding labels\n",
    "print(t3.shape)\n",
    "print(X_noise.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173104, 27, 19)\n",
      "(173104, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.concatenate((X_music, X_speech, X_noise), axis = 0)       #concatenating music, speech and silence samples\n",
    "t_train = np.concatenate((t1, t2, t3), axis = 0)\n",
    "print(X_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(128, activation = 'relu', return_sequences = True))     #RNN layer \n",
    "model.add(Dropout(0.2))   # Prevents overfitting\n",
    "\n",
    "model.add(LSTM(128, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(3, activation = 'softmax'))     #softmax for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate = 1e-3, decay = 1e-5)   #Adam is faster than stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])     #loss used for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173104/173104 [==============================] - 206s 1ms/sample - loss: 0.2384 - acc: 0.9089\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x25229badec8>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, t_train, epochs = 1, batch_size = 128)      # training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pathName = 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/test_samples/test_sample-0.npy'\n",
    "def print_cluster(pathName):\n",
    "    \n",
    "    Spect = readSpectrogram(pathName)     #function similar to createDataset\n",
    "    Speech = normalize(Spect)\n",
    "    imageNo = Speech.shape[1]\n",
    "    X = np.zeros((imageNo, 27, 19))\n",
    "    for i in range(imageNo):\n",
    "        Image = Speech[:, i]\n",
    "        Image = Image.reshape((27, 19))\n",
    "        X[i] = Image\n",
    "\n",
    "    X_test = X\n",
    "\n",
    "    classes = ['Music', 'Speech', 'Silence']\n",
    "    cluster = []\n",
    "\n",
    "    for i in range(X_test.shape[0]):\n",
    "        X_temp = X_test[i]\n",
    "        X_temp = np.expand_dims(X_temp, axis=0)    # Since RNN accepts 4 dimensions\n",
    "        arr = model.predict(X_temp)          #Predictions array\n",
    "        cluster.append(classes[np.argmax(arr)])     # Select that class whose probability is most and append it to cluster\n",
    "\n",
    "    return cluster   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audioEventDetect(cluster, window_size = 0.064):     #initial event detection method\n",
    "    super_list = []      #stores all events\n",
    "    cur_list = []        #stores start time, end time and event class\n",
    "    cur_list.append(0)\n",
    "    cur_list.append(window_size)     # end time of first event\n",
    "    cur_class = cluster[0]\n",
    "    cur_list.append(cur_class)\n",
    "    super_list.append(cur_list)\n",
    "    prev_class = cur_class      \n",
    "    cluster = cluster[1:]        #start iterating from 2nd element of cluster\n",
    "    \n",
    "    for cur_class in cluster:\n",
    "        if cur_class == prev_class:     #eg. if music is followed by music \n",
    "            super_list[-1][1] += window_size - 0.032    #end time of current event is increased by window_size - 0.032s\n",
    "        else:\n",
    "            cur_list = super_list[-1]    \n",
    "            new_start = cur_list[1]      #start time of new event is end time of previous event\n",
    "            cur_list = [new_start]\n",
    "            cur_list.append((new_start + window_size - 0.032))     # end time of new event is new start event + remaining part of overlapping window\n",
    "            cur_list.append(cur_class)\n",
    "            super_list.append(cur_list)\n",
    "        prev_class = cur_class\n",
    "                            \n",
    "    return super_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Dhruv Goyal/Desktop/mocktest_set/spectrogram\\test_sample-0.npy\n",
      "['Music', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Speech', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Silence', 'Speech', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Silence', 'Music', 'Silence', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Speech', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Speech', 'Speech', 'Music', 'Music', 'Music', 'Music', 'Music', 'Speech', 'Music', 'Music', 'Music', 'Music', 'Speech', 'Music', 'Music', 'Music', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Music', 'Speech', 'Silence']\n",
      "16\n",
      "1\n",
      "0\n",
      "C:/Users/Dhruv Goyal/Desktop/mocktest_set/spectrogram\\test_sample-1.npy\n",
      "0\n",
      "0\n",
      "1\n",
      "C:/Users/Dhruv Goyal/Desktop/mocktest_set/spectrogram\\test_sample-2.npy\n",
      "8\n",
      "1\n",
      "0\n",
      "C:/Users/Dhruv Goyal/Desktop/mocktest_set/spectrogram\\test_sample-3.npy\n",
      "3\n",
      "0\n",
      "1\n",
      "C:/Users/Dhruv Goyal/Desktop/mocktest_set/spectrogram\\test_sample-4.npy\n",
      "6\n",
      "0\n",
      "1\n",
      "C:/Users/Dhruv Goyal/Desktop/mocktest_set/spectrogram\\test_sample-5.npy\n",
      "1\n",
      "0\n",
      "1\n",
      "C:/Users/Dhruv Goyal/Desktop/mocktest_set/spectrogram\\test_sample-6.npy\n",
      "6\n",
      "0\n",
      "1\n",
      "C:/Users/Dhruv Goyal/Desktop/mocktest_set/spectrogram\\test_sample-7.npy\n",
      "1\n",
      "0\n",
      "1\n",
      "C:/Users/Dhruv Goyal/Desktop/mocktest_set/spectrogram\\test_sample-8.npy\n",
      "6\n",
      "0\n",
      "1\n",
      "C:/Users/Dhruv Goyal/Desktop/mocktest_set/spectrogram\\test_sample-9.npy\n",
      "82\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "\n",
    "header1 = ['filename','event','onset','offset']\n",
    "header2 = ['filename','Music','Speech']\n",
    "countt = 0\n",
    "\n",
    "file_paths = glob.glob('C:/Users/Dhruv Goyal/Desktop/mocktest_set/spectrogram' + '/**/*.npy',recursive = True)\n",
    "for pathname in file_paths:\n",
    "    filename = 'test_sample-' + str(countt)\n",
    "    cluster = print_cluster(pathname)\n",
    "#     print(cluster)\n",
    "    events = audioEventDetect(cluster)\n",
    "    \n",
    "    noOfFrames = 29        # this functions does a majority voting on 28 (sub)frames and allots corresponding label to this new frame (0.96 s in length) of 28 (sub)frames \n",
    "    new_cluster = []\n",
    "    count = 0 \n",
    "    music_score = 0\n",
    "    speech_score = 0\n",
    "    silence_score = 0\n",
    "    for title in cluster:     # count number of music, speech, or silence classes in 28 frames  \n",
    "        if title == 'Music':\n",
    "            music_score+= 1    \n",
    "        elif title == 'Speech':\n",
    "            speech_score+= 1 \n",
    "        else:\n",
    "            silence_score += 1\n",
    "        if count == noOfFrames-1:     # iterate till 28 frames have been counted \n",
    "            count = -1\n",
    "            if music_score > speech_score:\n",
    "                if music_score > silence_score:\n",
    "                    new_cluster.append('Music')    # max class is Music\n",
    "                else:\n",
    "                    new_cluster.append('Silence')   #max class is Silence.. and so on\n",
    "            else:\n",
    "                if speech_score > silence_score:\n",
    "                    new_cluster.append('Speech')\n",
    "                else:\n",
    "                    new_cluster.append('Silence')\n",
    "            music_score = 0\n",
    "            speech_score = 0\n",
    "            silence_score = 0\n",
    "        count+= 1\n",
    "        \n",
    "    events = audioEventDetect(new_cluster, 0.96)       #pass this new cluster of 0.96s frames through audio event detection method\n",
    "    for event in events:\n",
    "        if event[-1] == 'Silence':\n",
    "            events.remove(event)\n",
    "    for event in events:\n",
    "        data = [filename]\n",
    "        data.append(event[2])\n",
    "        data.append(event[0])\n",
    "        data.append(event[1])\n",
    "        with open('C:/Users/Dhruv Goyal/Desktop/event identification.csv', 'a',newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(data) \n",
    "    \n",
    "#     print(events)\n",
    "    m = 0\n",
    "    s = 0\n",
    "    for event in cluster:\n",
    "        if event == 'Music':\n",
    "            m=m+1\n",
    "        elif event == 'Speech':\n",
    "            s=s+1\n",
    "    print(s)\n",
    "    if m > 50:\n",
    "        m = 1\n",
    "    else:\n",
    "        m =0\n",
    "    if s > 50:\n",
    "        s = 1\n",
    "    else:\n",
    "        s =0\n",
    "    if m == 0:\n",
    "        s=1\n",
    "    data = [filename]\n",
    "    data.append(m)\n",
    "    data.append(s)\n",
    "    with open('C:/Users/Dhruv Goyal/Desktop/event tagging.csv', 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(data)\n",
    "    countt = countt + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "path = 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/CNN_Model/my_model_CNN.h5'\n",
    "path1 = 'C:/Users/Videh Aggarwal/Downloads/my_model_RNN.h5'\n",
    "model.save(path)                 # save model\n",
    "#model = load_model(path)        # load model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
