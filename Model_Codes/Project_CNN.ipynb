{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\Dhruv Goyal\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Dhruv Goyal\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Dhruv Goyal\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Dhruv Goyal\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Dhruv Goyal\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Dhruv Goyal\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\Dhruv Goyal\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\Dhruv Goyal\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\Dhruv Goyal\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\Dhruv Goyal\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\Dhruv Goyal\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\Dhruv Goyal\\anaconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'divide': 'warn', 'over': 'warn', 'under': 'ignore', 'invalid': 'warn'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import keras\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Conv2D, MaxPooling2D, LSTM, Reshape\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from sklearn import preprocessing\n",
    "from keras.layers.core import Reshape\n",
    "np.seterr(divide='ignore', invalid='ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAudio(filename):\n",
    "    x, sr = librosa.load(filename, sr=16000)\n",
    "    return x, sr\n",
    "\n",
    "#calculate spectrogram\n",
    "def calc_spec(x):\n",
    "    n_fft = 1024\n",
    "    hop_length = 512\n",
    "    win_length = 1024\n",
    "    X = np.abs(librosa.stft(x, n_fft = n_fft, hop_length = hop_length, win_length = win_length, window='hann', dtype = np.clongdouble))\n",
    "    X = librosa.power_to_db(X**2,ref=np.max)\n",
    "    return X\n",
    "\n",
    "def saveSpectrogram(X, outfilename):\n",
    "    assert outfilename[-4:]=='.npy'  #'outfilename extension should be .npy'\n",
    "    np.save(outfilename, X)\n",
    "    return\n",
    "\n",
    "def readSpectrogram(infilename):\n",
    "    X = np.load(infilename)\n",
    "    return X\n",
    "\n",
    "nmfcc = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr, features = 513):             # used to scale all values between 0 and 1\n",
    "    arr_max = np.ndarray.max(arr, axis = 1)      #obtaining max and min values for each feature\n",
    "    arr_min = np.ndarray.min(arr, axis = 1)\n",
    "    arr_max = arr_max.reshape((features, 1))\n",
    "    arr_min = arr_min.reshape((features, 1))\n",
    "    diff = arr_max - arr_min                    #getting the range of each value\n",
    "    arr = arr- arr_min\n",
    "    arr = arr/diff \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createDataset(dirname):            # obtain 3d array (samples, 27, 19)                        \n",
    "    speech, sr = readAudio(dirname)\n",
    "    Speech = calc_spec(speech)          #obtain Spectrogram\n",
    "    Speech = normalize(Speech)\n",
    "    imageNo = Speech.shape[1]         #Number of samples (frames)\n",
    "    X = np.zeros((imageNo, 27, 19))    \n",
    "    for i in range(imageNo):\n",
    "        Image = Speech[:, i]                # storing 513 features into Image \n",
    "        Image = Image.reshape((27, 19))       # reshaping 513 = 27*19 features\n",
    "        X[i] = Image     \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'C:/Users/Dhruv Goyal/Desktop/Ye Khol/music2.wav'\n",
    "X_music = createDataset(dirname)     #Music Dataset\n",
    "imageNo = X_music.shape[0]\n",
    "t1 =  np.array([[1,0,0]]*imageNo)    #Corresponding ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(54392, 3)\n",
      "54392\n"
     ]
    }
   ],
   "source": [
    "dirname = 'C:/Users/Dhruv Goyal/Desktop/Ye Khol/Speech2.wav'\n",
    "X_speech = createDataset(dirname)      #Speech Dataset\n",
    "imageNo = X_speech.shape[0]\n",
    "t2 =  np.array([[0,1,0]]*imageNo)  #Corresponding Ground Truths\n",
    "print(t2.shape)\n",
    "print(X_speech.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(62501, 3)\n",
      "62501\n"
     ]
    }
   ],
   "source": [
    "dirname = 'C:/Users/Dhruv Goyal/Desktop/Ye Khol/noise2.wav'\n",
    "X_noise = createDataset(dirname)       #Silence Dataset\n",
    "imageNo = X_noise.shape[0]\n",
    "t3 =  np.array([[0,0,1]]*imageNo)      #Corresponding labels\n",
    "print(t3.shape)\n",
    "print(X_noise.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(173104, 27, 19, 1)\n",
      "(173104, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.concatenate((X_music, X_speech, X_noise), axis = 0)       #concatenating music, speech and silence samples\n",
    "X_train = np.expand_dims(X_train, axis=3)                         #adding a 4th dim since CNN models accept 4 dimensions\n",
    "t_train = np.concatenate((t1, t2, t3), axis = 0)\n",
    "print(X_train.shape)\n",
    "print(t_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Dhruv Goyal\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu', input_shape = X_train.shape[1:]))   #convolution layers with kernel size of (3, 3)\n",
    "model.add(Dropout(0.2))      #prevent overfitting\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu'))    \n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())    # convert data to vector which can be passed to dense layer\n",
    "\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Dense(3, activation = 'softmax'))    #softmax for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate = 1e-3, decay = 1e-5)    #Adam is faster than stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'categorical_crossentropy', optimizer = opt, metrics = ['accuracy'])    #loss used for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "173104/173104 [==============================] - 438s 3ms/sample - loss: 0.0528 - acc: 0.9835\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x23a0e9c40c8>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, t_train, epochs = 1, batch_size = 128)     #training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pathName = 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/test_samples/test_sample-0.npy'\n",
    "\n",
    "def print_cluster(pathName):\n",
    "    Spect = readSpectrogram(pathName)     #function similar to createDataset\n",
    "    Speech = normalize(Spect)\n",
    "    imageNo = Speech.shape[1]\n",
    "    X = np.zeros((imageNo, 27, 19))\n",
    "    for i in range(imageNo):\n",
    "        Image = Speech[:, i]\n",
    "        Image = Image.reshape((27, 19))\n",
    "        X[i] = Image\n",
    "\n",
    "    X_test = X\n",
    "\n",
    "    m_score = 0\n",
    "    speech_score = 0\n",
    "\n",
    "    classes = ['Music', 'Speech', 'Silence']\n",
    "    cluster = []\n",
    "\n",
    "    for i in range(X_test.shape[0]):\n",
    "        X_temp = X_test[i]\n",
    "#         print(X_temp.shape)\n",
    "        X_temp = np.expand_dims(X_temp, axis=0)    # Since CNN accepts 4 dimensions\n",
    "        X_temp = np.expand_dims(X_temp, axis=3) \n",
    "        arr = model.predict(X_temp)          #Predictions array\n",
    "        cluster.append(classes[np.argmax(arr)])     # Select that class whose probability is most and append it to cluster\n",
    "\n",
    "    return cluster   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audioEventDetect(cluster, window_size = 0.064):     #initial event detection method\n",
    "    super_list = []      #stores all events\n",
    "    cur_list = []        #stores start time, end time and event class\n",
    "    cur_list.append(0)\n",
    "    cur_list.append(window_size)     # end time of first event\n",
    "    cur_class = cluster[0]\n",
    "    cur_list.append(cur_class)\n",
    "    super_list.append(cur_list)\n",
    "    prev_class = cur_class      \n",
    "    cluster = cluster[1:]        #start iterating from 2nd element of cluster\n",
    "    \n",
    "    for cur_class in cluster:\n",
    "        if cur_class == prev_class:     #eg. if music is followed by music \n",
    "            super_list[-1][1] += window_size - 0.032    #end time of current event is increased by window_size - 0.032s\n",
    "        else:\n",
    "            cur_list = super_list[-1]    \n",
    "            new_start = cur_list[1]      #start time of new event is end time of previous event\n",
    "            cur_list = [new_start]\n",
    "            cur_list.append((new_start + window_size - 0.032))     # end time of new event is new start event + remaining part of overlapping window\n",
    "            cur_list.append(cur_class)\n",
    "            super_list.append(cur_list)\n",
    "        prev_class = cur_class\n",
    "                            \n",
    "    return super_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_sample-0\n",
      "[[6.528, 9.312000000000001, 'Music']]\n",
      "test_sample-1\n",
      "[]\n",
      "test_sample-2\n",
      "[[2.816, 3.7439999999999998, 'Music'], [4.672, 5.6, 'Music']]\n",
      "test_sample-3\n",
      "[[0, 2.816, 'Music'], [5.6, 7.4559999999999995, 'Music'], [8.384, 9.312000000000001, 'Music']]\n",
      "test_sample-4\n",
      "[[0, 9.312000000000001, 'Music']]\n",
      "test_sample-5\n",
      "[]\n",
      "test_sample-6\n",
      "[[0, 9.312000000000001, 'Music']]\n",
      "test_sample-7\n",
      "[]\n",
      "test_sample-8\n",
      "[[0, 9.312000000000001, 'Music']]\n",
      "test_sample-9\n",
      "[[1.888, 3.7439999999999998, 'Speech'], [6.528, 8.384, 'Speech'], [8.384, 9.312000000000001, 'Music']]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import csv\n",
    "\n",
    "header1 = ['filename','event','onset','offset']\n",
    "header2 = ['filename','Music','Speech']\n",
    "countt = 0\n",
    "\n",
    "file_paths = glob.glob('C:/Users/Dhruv Goyal/Desktop/mocktest_set/spectrogram' + '/**/*.npy',recursive = True)\n",
    "for pathname in file_paths:\n",
    "    filename = 'test_sample-' + str(countt)\n",
    "    cluster = print_cluster(pathname)\n",
    "#     print(cluster)\n",
    "    events = audioEventDetect(cluster)\n",
    "    \n",
    "    noOfFrames = 29        # this functions does a majority voting on 28 (sub)frames and allots corresponding label to this new frame (0.96 s in length) of 28 (sub)frames \n",
    "    new_cluster = []\n",
    "    count = 0 \n",
    "    music_score = 0\n",
    "    speech_score = 0\n",
    "    silence_score = 0\n",
    "    for title in cluster:     # count number of music, speech, or silence classes in 28 frames  \n",
    "        if title == 'Music':\n",
    "            music_score+= 1    \n",
    "        elif title == 'Speech':\n",
    "            speech_score+= 1 \n",
    "        else:\n",
    "            silence_score += 1\n",
    "        if count == noOfFrames-1:     # iterate till 28 frames have been counted \n",
    "            count = -1\n",
    "            if music_score > speech_score:\n",
    "                if music_score > silence_score:\n",
    "                    new_cluster.append('Music')    # max class is Music\n",
    "                else:\n",
    "                    new_cluster.append('Silence')   #max class is Silence.. and so on\n",
    "            else:\n",
    "                if speech_score > silence_score:\n",
    "                    new_cluster.append('Speech')\n",
    "                else:\n",
    "                    new_cluster.append('Silence')\n",
    "            music_score = 0\n",
    "            speech_score = 0\n",
    "            silence_score = 0 \n",
    "        count+= 1\n",
    "        \n",
    "    events = audioEventDetect(new_cluster, 0.96)       #pass this new cluster of 0.96s frames through audio event detection method\n",
    "    for event in events:\n",
    "        if event[-1] == 'Silence':\n",
    "            events.remove(event)\n",
    "    print(filename)\n",
    "    print(events)\n",
    "    for event in events:\n",
    "        data = [filename]\n",
    "        data.append(event[2])\n",
    "        data.append(event[0])\n",
    "        data.append(event[1])\n",
    "        with open('C:/Users/Dhruv Goyal/Desktop/event identification.csv', 'a',newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow(data)  \n",
    "    \n",
    "#     print(events)\n",
    "    m = 0\n",
    "    s = 0\n",
    "    for event in cluster:\n",
    "        if event == 'Music':\n",
    "            m=m+1\n",
    "        elif event == 'Speech':\n",
    "            s=s+1\n",
    "    if m > 100:\n",
    "        m = 1\n",
    "    else:\n",
    "        m =0\n",
    "    if s > 100:\n",
    "        s = 1\n",
    "    else:\n",
    "        s =0\n",
    "    if m == 0:\n",
    "        s=1\n",
    "    \n",
    "    data = [filename]\n",
    "    data.append(m)\n",
    "    data.append(s)\n",
    "    with open('C:/Users/Dhruv Goyal/Desktop/event tagging.csv', 'a', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(data)\n",
    "    countt = countt + 1\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "path = 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/CNN_Model/my_model_CNN.h5'\n",
    "path1 = 'C:/Users/Videh Aggarwal/Downloads/my_model_RNN.h5'\n",
    "model.save(path)                 # save model\n",
    "#model = load_model(path)        # load model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
