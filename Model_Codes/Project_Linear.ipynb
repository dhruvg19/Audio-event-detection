{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readAudio(filename):\n",
    "    x, sr = librosa.load(filename, sr=16000)\n",
    "    return x, sr\n",
    "\n",
    "#calculate spectrogram\n",
    "def calc_spec(x):\n",
    "    n_fft = 1024\n",
    "    hop_length = 512\n",
    "    win_length = 1024\n",
    "    X = np.abs(librosa.stft(x, n_fft = n_fft, hop_length = hop_length, win_length = win_length, window='hann', dtype = np.clongdouble))\n",
    "    X = librosa.power_to_db(X**2,ref=np.max)\n",
    "    return X\n",
    "\n",
    "def saveSpectrogram(X, outfilename):\n",
    "    assert outfilename[-4:]=='.npy'  #'outfilename extension should be .npy'\n",
    "    np.save(outfilename, X)\n",
    "    return\n",
    "\n",
    "def readSpectrogram(infilename):\n",
    "    X = np.load(infilename)\n",
    "    return X\n",
    "\n",
    "nmfcc = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(arr):          # used to scale all values between 0 and 1\n",
    "    arr_max = np.ndarray.max(arr, axis = 1)    # obtain max and min values for each feature   \n",
    "    arr_min = np.ndarray.min(arr, axis = 1)\n",
    "    arr_max = arr_max.reshape((nmfcc, 1))\n",
    "    arr_min = arr_min.reshape((nmfcc, 1))\n",
    "    diff = arr_max - arr_min         # obtain range of each feature\n",
    "    arr = arr-arr_min\n",
    "    arr = arr/diff\n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/Datasets/MusicData/music2.wav' #Music file\n",
    "audio, sr = readAudio(dirname)\n",
    "log_mels = calc_spec(audio)          #calculate spectrograms\n",
    "music_mfcc = librosa.feature.mfcc(S = log_mels, sr=16000, n_mfcc=13)    #extract mfcc features\n",
    "music_mfcc = normalize(music_mfcc).T\n",
    "imageNo = music_mfcc.shape[0]\n",
    "t1 =  np.array([[1,0,0]]*imageNo)    #Corresponding ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeating above process for speech data\n",
    "dirname = 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/Datasets/SpeechData/Speech2.wav'\n",
    "audio, sr = readAudio(dirname)\n",
    "log_mels = calc_spec(audio)\n",
    "speech_mfcc = librosa.feature.mfcc(S = log_mels, sr=16000, n_mfcc=13)\n",
    "speech_mfcc = normalize(speech_mfcc).T\n",
    "imageNo = speech_mfcc.shape[0]\n",
    "t2 =  np.array([[0,1,0]]*imageNo)    #Corresponding ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Repeating above process for silence data\n",
    "dirname = 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/Datasets/NoiseData/noise2.wav'\n",
    "audio, sr = readAudio(dirname)\n",
    "log_mels = calc_spec(audio)\n",
    "silence_mfcc = librosa.feature.mfcc(S = log_mels, sr=16000, n_mfcc=13)\n",
    "silence_mfcc = normalize(silence_mfcc).T\n",
    "imageNo = silence_mfcc.shape[0]\n",
    "t3 =  np.array([[0,0,1]]*imageNo)    #Corresponding ground truths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_temp = np.concatenate((music_mfcc, speech_mfcc, silence_mfcc))  #creating X transpose matrix\n",
    "total_samples = music_mfcc.shape[0] + speech_mfcc.shape[0] + silence_mfcc.shape[0]\n",
    "ones_arr = np.ones((total_samples, 1))\n",
    "phi = np.concatenate((ones_arr, X_train_temp), axis = 1)    #attaching bias terms to X transpose to make final phi matrix\n",
    "Y = np.concatenate((t1, t2, t3))     #concatenated ground truths\n",
    "W = np.random.rand(phi.shape[1], 3)  #initializing random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.54335848  0.10009325 -1.81148428]\n",
      " [ 1.9359647  -1.18391543 -1.26776529]\n",
      " [ 2.51941079 -1.85292736 -1.26013106]\n",
      " [-1.45123275  0.78631498  0.96253142]\n",
      " [-1.57111171  0.7767184   1.24294347]\n",
      " [-0.12731203 -0.24028291  0.53429718]\n",
      " [-0.77238113  0.55910096  0.29485656]\n",
      " [ 0.09843508 -0.08736832  0.17950085]\n",
      " [-1.76490217  1.11618789  0.99406246]\n",
      " [ 2.40973086 -1.3416438  -1.18201635]\n",
      " [-0.45810146  0.72721771  0.0903639 ]\n",
      " [-0.67896434 -0.93150197  2.11603657]\n",
      " [-0.748221    0.58593803  0.65772563]\n",
      " [-2.71194598  1.88570312  1.62514838]]\n"
     ]
    }
   ],
   "source": [
    "# Training step\n",
    "# We know that W = (inv(phi.T*phi + lamda*I))* phi.T * Y\n",
    "\n",
    "temp_phi = np.dot(phi.T, phi)\n",
    "lamda = 0.02        #regularization parameter\n",
    "temp_phi += lamda*np.eye(phi.shape[1])\n",
    "\n",
    "#inverting above matrix\n",
    "a = temp_phi\n",
    "m = 10^-6\n",
    "inv_prod = np.linalg.inv(a+ np.eye(a.shape[1])*m)     #small noise added in diagonals to prevent singularity\n",
    "\n",
    "temp = np.dot(inv_prod, phi.T)\n",
    "W = np.dot(temp, Y)      #final weights matrix\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Speech', 'Speech', 'Silence', 'Speech', 'Speech', 'Silence', 'Speech', 'Silence', 'Speech', 'Silence', 'Speech', 'Silence', 'Silence', 'Speech', 'Speech', 'Speech', 'Silence', 'Silence', 'Silence', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Silence', 'Silence', 'Silence', 'Speech', 'Silence', 'Silence', 'Speech', 'Music', 'Silence', 'Speech', 'Speech', 'Speech', 'Silence', 'Speech', 'Speech', 'Silence', 'Silence', 'Music', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Speech', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Music', 'Silence', 'Silence', 'Speech', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Speech', 'Silence', 'Silence', 'Silence', 'Silence', 'Speech', 'Silence', 'Silence', 'Silence', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Silence', 'Speech', 'Silence', 'Silence', 'Speech', 'Silence', 'Silence', 'Silence', 'Speech', 'Silence', 'Silence', 'Silence', 'Speech', 'Speech', 'Speech', 'Silence', 'Speech', 'Silence', 'Speech', 'Speech', 'Speech', 'Speech', 'Silence', 'Silence', 'Silence', 'Speech', 'Speech', 'Speech', 'Silence', 'Silence', 'Silence', 'Silence', 'Speech', 'Speech', 'Silence', 'Silence', 'Speech', 'Music', 'Music', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Speech', 'Silence', 'Music', 'Music', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Music', 'Silence', 'Music', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Speech', 'Speech', 'Music', 'Music', 'Silence', 'Silence', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Music', 'Silence', 'Silence', 'Speech', 'Speech', 'Speech', 'Silence', 'Speech', 'Speech', 'Silence', 'Speech', 'Silence', 'Silence', 'Silence', 'Speech', 'Speech', 'Silence', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Silence', 'Silence', 'Speech', 'Silence', 'Speech', 'Speech', 'Silence', 'Silence', 'Speech', 'Speech', 'Silence', 'Silence', 'Silence', 'Speech', 'Speech', 'Speech', 'Silence', 'Silence', 'Speech', 'Silence', 'Speech', 'Speech', 'Silence', 'Silence', 'Speech', 'Silence', 'Speech', 'Silence', 'Silence', 'Silence', 'Speech', 'Silence', 'Speech', 'Speech', 'Silence', 'Speech', 'Silence', 'Speech', 'Silence', 'Silence', 'Speech', 'Music', 'Speech', 'Speech', 'Music', 'Music', 'Music', 'Speech', 'Music', 'Speech', 'Music', 'Speech', 'Silence', 'Music', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Music', 'Speech', 'Silence', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Silence', 'Speech', 'Silence', 'Silence', 'Silence', 'Speech', 'Silence', 'Speech', 'Silence', 'Silence', 'Speech', 'Speech', 'Silence', 'Speech', 'Speech', 'Silence', 'Speech', 'Speech', 'Silence', 'Speech', 'Silence', 'Silence', 'Silence', 'Speech', 'Speech', 'Silence', 'Speech', 'Silence', 'Speech', 'Silence', 'Silence', 'Silence', 'Silence', 'Speech', 'Speech', 'Silence', 'Speech', 'Speech', 'Speech', 'Silence', 'Speech', 'Speech', 'Speech', 'Silence', 'Speech', 'Silence', 'Speech', 'Speech', 'Speech', 'Speech', 'Silence', 'Silence', 'Silence', 'Silence', 'Silence', 'Speech', 'Speech', 'Speech', 'Speech', 'Speech', 'Silence', 'Speech', 'Silence']\n"
     ]
    }
   ],
   "source": [
    "dirname = 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/val_set/val_set/wav/music_noisy1.wav'\n",
    "x_test, sr = readAudio(dirname)\n",
    "log_mels = calc_spec(x_test)    # in test, these spectrograms would be given\n",
    "mfcc = librosa.feature.mfcc(S = log_mels, sr=16000, n_mfcc=13)   # extract mfcc features from spectrogram\n",
    "X_test = normalize(mfcc).T\n",
    "ones_arr = np.ones((X_test.shape[0], 1))\n",
    "phi = np.concatenate((ones_arr, X_test), axis = 1)     #attaching bias terms to X transpose to make final phi matrix\n",
    "Y = np.dot(phi, W)   # predictions of model \n",
    "\n",
    "classes = ['Music', 'Speech', 'Silence']\n",
    "cluster = []\n",
    "for i in range(Y.shape[0]):\n",
    "    pred = Y[i]         #individual prediction array\n",
    "    cluster.append(classes[np.argmax(pred)])     # append the class with max probability\n",
    "        \n",
    "print(cluster)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audioEventDetect(cluster, window_size = 0.064):     #initial event detection method\n",
    "    super_list = []      #stores all events\n",
    "    cur_list = []        #stores start time, end time and event class\n",
    "    cur_list.append(0)\n",
    "    cur_list.append(window_size)     # end time of first event\n",
    "    cur_class = cluster[0]\n",
    "    cur_list.append(cur_class)\n",
    "    super_list.append(cur_list)\n",
    "    prev_class = cur_class      \n",
    "    cluster = cluster[1:]        #start iterating from 2nd element of cluster\n",
    "    \n",
    "    for cur_class in cluster:\n",
    "        if cur_class == prev_class:     #eg. if music is followed by music \n",
    "            super_list[-1][1] += window_size - 0.032    #end time of current event is increased by window_size - 0.032s\n",
    "        else:\n",
    "            cur_list = super_list[-1]    \n",
    "            new_start = cur_list[1]      #start time of new event is end time of previous event\n",
    "            cur_list = [new_start]\n",
    "            cur_list.append((new_start + window_size - 0.032))     # end time of new event is new start event + remaining part of overlapping window\n",
    "            cur_list.append(cur_class)\n",
    "            super_list.append(cur_list)\n",
    "        prev_class = cur_class\n",
    "                            \n",
    "    return super_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0.096, 'Speech'], [0.096, 0.128, 'Silence'], [0.128, 0.192, 'Speech'], [0.192, 0.224, 'Silence'], [0.224, 0.256, 'Speech'], [0.256, 0.28800000000000003, 'Silence'], [0.28800000000000003, 0.32000000000000006, 'Speech'], [0.32000000000000006, 0.3520000000000001, 'Silence'], [0.3520000000000001, 0.3840000000000001, 'Speech'], [0.3840000000000001, 0.4480000000000002, 'Silence'], [0.4480000000000002, 0.5440000000000003, 'Speech'], [0.5440000000000003, 0.6400000000000003, 'Silence'], [0.6400000000000003, 0.8000000000000005, 'Speech'], [0.8000000000000005, 0.8960000000000006, 'Silence'], [0.8960000000000006, 0.9280000000000006, 'Speech'], [0.9280000000000006, 0.9920000000000007, 'Silence'], [0.9920000000000007, 1.0240000000000007, 'Speech'], [1.0240000000000007, 1.0560000000000007, 'Music'], [1.0560000000000007, 1.0880000000000007, 'Silence'], [1.0880000000000007, 1.1840000000000008, 'Speech'], [1.1840000000000008, 1.2160000000000009, 'Silence'], [1.2160000000000009, 1.280000000000001, 'Speech'], [1.280000000000001, 1.344000000000001, 'Silence'], [1.344000000000001, 1.376000000000001, 'Music'], [1.376000000000001, 1.5360000000000011, 'Silence'], [1.5360000000000011, 1.5680000000000012, 'Speech'], [1.5680000000000012, 1.7280000000000013, 'Silence'], [1.7280000000000013, 1.7600000000000013, 'Music'], [1.7600000000000013, 1.8240000000000014, 'Silence'], [1.8240000000000014, 1.8560000000000014, 'Speech'], [1.8560000000000014, 2.0480000000000014, 'Silence'], [2.0480000000000014, 2.0800000000000014, 'Speech'], [2.0800000000000014, 2.2080000000000015, 'Silence'], [2.2080000000000015, 2.2400000000000015, 'Speech'], [2.2400000000000015, 2.3360000000000016, 'Silence'], [2.3360000000000016, 2.4960000000000018, 'Speech'], [2.4960000000000018, 2.528000000000002, 'Silence'], [2.528000000000002, 2.560000000000002, 'Speech'], [2.560000000000002, 2.624000000000002, 'Silence'], [2.624000000000002, 2.656000000000002, 'Speech'], [2.656000000000002, 2.752000000000002, 'Silence'], [2.752000000000002, 2.784000000000002, 'Speech'], [2.784000000000002, 2.880000000000002, 'Silence'], [2.880000000000002, 2.976000000000002, 'Speech'], [2.976000000000002, 3.0080000000000022, 'Silence'], [3.0080000000000022, 3.0400000000000023, 'Speech'], [3.0400000000000023, 3.0720000000000023, 'Silence'], [3.0720000000000023, 3.2000000000000024, 'Speech'], [3.2000000000000024, 3.2960000000000025, 'Silence'], [3.2960000000000025, 3.3920000000000026, 'Speech'], [3.3920000000000026, 3.5200000000000027, 'Silence'], [3.5200000000000027, 3.5840000000000027, 'Speech'], [3.5840000000000027, 3.648000000000003, 'Silence'], [3.648000000000003, 3.680000000000003, 'Speech'], [3.680000000000003, 3.744000000000003, 'Music'], [3.744000000000003, 3.968000000000003, 'Silence'], [3.968000000000003, 4.000000000000003, 'Speech'], [4.000000000000003, 4.032000000000003, 'Silence'], [4.032000000000003, 4.096000000000003, 'Music'], [4.096000000000003, 4.256000000000003, 'Silence'], [4.256000000000003, 4.288000000000003, 'Music'], [4.288000000000003, 4.320000000000003, 'Silence'], [4.320000000000003, 4.352000000000003, 'Music'], [4.352000000000003, 4.544000000000003, 'Silence'], [4.544000000000003, 4.608000000000003, 'Speech'], [4.608000000000003, 4.672000000000003, 'Music'], [4.672000000000003, 4.736000000000003, 'Silence'], [4.736000000000003, 5.024000000000004, 'Music'], [5.024000000000004, 5.088000000000004, 'Silence'], [5.088000000000004, 5.184000000000004, 'Speech'], [5.184000000000004, 5.216000000000004, 'Silence'], [5.216000000000004, 5.280000000000004, 'Speech'], [5.280000000000004, 5.312000000000004, 'Silence'], [5.312000000000004, 5.344000000000004, 'Speech'], [5.344000000000004, 5.440000000000004, 'Silence'], [5.440000000000004, 5.504000000000004, 'Speech'], [5.504000000000004, 5.536000000000004, 'Silence'], [5.536000000000004, 5.696000000000004, 'Speech'], [5.696000000000004, 5.760000000000004, 'Silence'], [5.760000000000004, 5.792000000000004, 'Speech'], [5.792000000000004, 5.824000000000004, 'Silence'], [5.824000000000004, 5.888000000000004, 'Speech'], [5.888000000000004, 5.952000000000004, 'Silence'], [5.952000000000004, 6.0160000000000045, 'Speech'], [6.0160000000000045, 6.1120000000000045, 'Silence'], [6.1120000000000045, 6.208000000000005, 'Speech'], [6.208000000000005, 6.272000000000005, 'Silence'], [6.272000000000005, 6.304000000000005, 'Speech'], [6.304000000000005, 6.336000000000005, 'Silence'], [6.336000000000005, 6.400000000000005, 'Speech'], [6.400000000000005, 6.464000000000005, 'Silence'], [6.464000000000005, 6.496000000000005, 'Speech'], [6.496000000000005, 6.528000000000005, 'Silence'], [6.528000000000005, 6.560000000000005, 'Speech'], [6.560000000000005, 6.656000000000005, 'Silence'], [6.656000000000005, 6.688000000000005, 'Speech'], [6.688000000000005, 6.720000000000005, 'Silence'], [6.720000000000005, 6.784000000000005, 'Speech'], [6.784000000000005, 6.816000000000005, 'Silence'], [6.816000000000005, 6.848000000000005, 'Speech'], [6.848000000000005, 6.880000000000005, 'Silence'], [6.880000000000005, 6.912000000000005, 'Speech'], [6.912000000000005, 6.976000000000005, 'Silence'], [6.976000000000005, 7.008000000000005, 'Speech'], [7.008000000000005, 7.040000000000005, 'Music'], [7.040000000000005, 7.104000000000005, 'Speech'], [7.104000000000005, 7.2000000000000055, 'Music'], [7.2000000000000055, 7.2320000000000055, 'Speech'], [7.2320000000000055, 7.264000000000006, 'Music'], [7.264000000000006, 7.296000000000006, 'Speech'], [7.296000000000006, 7.328000000000006, 'Music'], [7.328000000000006, 7.360000000000006, 'Speech'], [7.360000000000006, 7.392000000000006, 'Silence'], [7.392000000000006, 7.424000000000006, 'Music'], [7.424000000000006, 7.680000000000006, 'Speech'], [7.680000000000006, 7.712000000000006, 'Music'], [7.712000000000006, 7.744000000000006, 'Speech'], [7.744000000000006, 7.776000000000006, 'Silence'], [7.776000000000006, 8.032000000000005, 'Speech'], [8.032000000000005, 8.064000000000005, 'Silence'], [8.064000000000005, 8.096000000000005, 'Speech'], [8.096000000000005, 8.192000000000005, 'Silence'], [8.192000000000005, 8.224000000000006, 'Speech'], [8.224000000000006, 8.256000000000006, 'Silence'], [8.256000000000006, 8.288000000000006, 'Speech'], [8.288000000000006, 8.352000000000006, 'Silence'], [8.352000000000006, 8.416000000000006, 'Speech'], [8.416000000000006, 8.448000000000006, 'Silence'], [8.448000000000006, 8.512000000000006, 'Speech'], [8.512000000000006, 8.544000000000006, 'Silence'], [8.544000000000006, 8.608000000000006, 'Speech'], [8.608000000000006, 8.640000000000006, 'Silence'], [8.640000000000006, 8.672000000000006, 'Speech'], [8.672000000000006, 8.768000000000006, 'Silence'], [8.768000000000006, 8.832000000000006, 'Speech'], [8.832000000000006, 8.864000000000006, 'Silence'], [8.864000000000006, 8.896000000000006, 'Speech'], [8.896000000000006, 8.928000000000006, 'Silence'], [8.928000000000006, 8.960000000000006, 'Speech'], [8.960000000000006, 9.088000000000006, 'Silence'], [9.088000000000006, 9.152000000000006, 'Speech'], [9.152000000000006, 9.184000000000006, 'Silence'], [9.184000000000006, 9.280000000000006, 'Speech'], [9.280000000000006, 9.312000000000006, 'Silence'], [9.312000000000006, 9.408000000000007, 'Speech'], [9.408000000000007, 9.440000000000007, 'Silence'], [9.440000000000007, 9.472000000000007, 'Speech'], [9.472000000000007, 9.504000000000007, 'Silence'], [9.504000000000007, 9.632000000000007, 'Speech'], [9.632000000000007, 9.792000000000007, 'Silence'], [9.792000000000007, 9.952000000000007, 'Speech'], [9.952000000000007, 9.984000000000007, 'Silence'], [9.984000000000007, 10.016000000000007, 'Speech'], [10.016000000000007, 10.048000000000007, 'Silence']]\n"
     ]
    }
   ],
   "source": [
    "events = audioEventDetect(cluster)\n",
    "print(events)                #initial audio event detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0.96, 'Speech'], [0.96, 9.312000000000001, 'Silence']]\n"
     ]
    }
   ],
   "source": [
    "noOfFrames = 29        # this functions does a majority voting on 28 (sub)frames and allots corresponding label to this new frame (0.96 s in length) of 28 (sub)frames \n",
    "new_cluster = []\n",
    "count = 0 \n",
    "music_score = 0\n",
    "speech_score = 0\n",
    "silence_score = 0\n",
    "for title in cluster:     # count number of music, speech, or silence classes in 28 frames  \n",
    "    if title == 'Music':\n",
    "        music_score+= 1    \n",
    "    elif title == 'Speech':\n",
    "        speech_score+= 1 \n",
    "    else:\n",
    "        silence_score += 1\n",
    "    if count == noOfFrames-1:     # iterate till 28 frames have been counted \n",
    "        count = -1\n",
    "        if music_score > speech_score:\n",
    "            if music_score > silence_score:\n",
    "                new_cluster.append('Music')    # max class is Music\n",
    "            else:\n",
    "                new_cluster.append('Silence')   #max class is Silence.. and so on\n",
    "        else:\n",
    "            if speech_score > silence_score:\n",
    "                new_cluster.append('Speech')\n",
    "            else:\n",
    "                new_cluster.append('Silence')\n",
    "        music_score = 0\n",
    "        speech_score = 0\n",
    "        silence_score = 0\n",
    "    count+= 1\n",
    "    \n",
    "events = audioEventDetect(new_cluster, 0.96)       #pass this new cluster of 0.96s frames through audio event detection method\n",
    "print(events)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirname = 'D:/Videh_Acads/IITK/5th Sem/EE603/Project/Model_Weights/linear_weights.npy'\n",
    "saveSpectrogram(W, dirname)        # saving model weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
